python train_vit.py --data_dir Datasets --epochs 10 --batch_size 4 --lr 5
e-5 --output_dir ./vit-customer-model
Using device: cuda
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ahmad10raza/anaconda3/envs/vit-gpu/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
{'eval_loss': 0.00022288854233920574, 'eval_accuracy': 1.0, 'eval_runtime': 19.9217, 'eval_samples_per_second': 10.34, 'eval_steps_per_second': 2.61, 'epoch': 1.0}                 
{'loss': 0.0985, 'grad_norm': 0.003974522929638624, 'learning_rate': 4.4623655913978496e-05, 'epoch': 1.08}                                                                         
{'eval_loss': 9.050134394783527e-05, 'eval_accuracy': 1.0, 'eval_runtime': 20.0833, 'eval_samples_per_second': 10.257, 'eval_steps_per_second': 2.589, 'epoch': 2.0}                
{'loss': 0.0001, 'grad_norm': 0.001171229174360633, 'learning_rate': 3.924731182795699e-05, 'epoch': 2.15}                                                                          
{'eval_loss': 5.3230003686621785e-05, 'eval_accuracy': 1.0, 'eval_runtime': 20.4098, 'eval_samples_per_second': 10.093, 'eval_steps_per_second': 2.548, 'epoch': 3.0}               
{'loss': 0.0001, 'grad_norm': 0.0009360804106108844, 'learning_rate': 3.387096774193548e-05, 'epoch': 3.23}                                                                         
{'eval_loss': 3.667347118607722e-05, 'eval_accuracy': 1.0, 'eval_runtime': 26.0821, 'eval_samples_per_second': 7.898, 'eval_steps_per_second': 1.994, 'epoch': 4.0}                 
{'loss': 0.0, 'grad_norm': 0.0006386191234923899, 'learning_rate': 2.8494623655913982e-05, 'epoch': 4.3}                                                                            
{'eval_loss': 2.7721522201318294e-05, 'eval_accuracy': 1.0, 'eval_runtime': 26.5973, 'eval_samples_per_second': 7.745, 'eval_steps_per_second': 1.955, 'epoch': 5.0}                
{'loss': 0.0, 'grad_norm': 0.00037920675822533667, 'learning_rate': 2.3118279569892472e-05, 'epoch': 5.38}                                                                          
{'eval_loss': 2.229648998763878e-05, 'eval_accuracy': 1.0, 'eval_runtime': 26.4029, 'eval_samples_per_second': 7.802, 'eval_steps_per_second': 1.969, 'epoch': 6.0}                 
{'loss': 0.0, 'grad_norm': 0.0003071926475968212, 'learning_rate': 1.774193548387097e-05, 'epoch': 6.45}                                                                            
{'eval_loss': 1.8871331121772528e-05, 'eval_accuracy': 1.0, 'eval_runtime': 26.4881, 'eval_samples_per_second': 7.777, 'eval_steps_per_second': 1.963, 'epoch': 7.0}                
{'loss': 0.0, 'grad_norm': 0.0006679092184640467, 'learning_rate': 1.2365591397849464e-05, 'epoch': 7.53}                                                                           
{'eval_loss': 1.668278309807647e-05, 'eval_accuracy': 1.0, 'eval_runtime': 26.3063, 'eval_samples_per_second': 7.831, 'eval_steps_per_second': 1.977, 'epoch': 8.0}                 
{'loss': 0.0, 'grad_norm': 0.00026812756550498307, 'learning_rate': 6.989247311827957e-06, 'epoch': 8.6}                                                                            
{'eval_loss': 1.5398700270452537e-05, 'eval_accuracy': 1.0, 'eval_runtime': 21.8893, 'eval_samples_per_second': 9.411, 'eval_steps_per_second': 2.376, 'epoch': 9.0}                
{'loss': 0.0, 'grad_norm': 0.00024208873219322413, 'learning_rate': 1.6129032258064516e-06, 'epoch': 9.68}                                                                          
{'eval_loss': 1.4945596376492176e-05, 'eval_accuracy': 1.0, 'eval_runtime': 25.1176, 'eval_samples_per_second': 8.201, 'eval_steps_per_second': 2.07, 'epoch': 10.0}                
{'train_runtime': 2617.8581, 'train_samples_per_second': 7.105, 'train_steps_per_second': 1.776, 'train_loss': 0.01062842070740918, 'epoch': 10.0}                                  
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4650/4650 [43:37<00:00,  1.78it/s]
Model saved at ./vit-customer-model/final